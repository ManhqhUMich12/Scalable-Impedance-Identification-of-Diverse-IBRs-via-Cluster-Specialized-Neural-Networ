{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754c908c"
      },
      "source": [
        "# Mixture of Experts for GFMI/GFLI Impedance Datasets\n",
        "\n",
        "This notebook loads multiple MATLAB `.mat` datasets (GFMI1/GFMI2, GFLI1/GFLI2/GFLI3),\n",
        "merges them, builds a simple **Mixture of Experts** (2 experts: GFMI & GFLI),\n",
        "trains on 70% of the data, tests on 30%, and plots evaluation results.\n",
        "\n",
        "**Assumptions**\n",
        "- Each `.mat` file contains a `Dataset` struct with fields:\n",
        "  - `X`  shaped `(N, 4)` = `[V_ref_pu, P_pu, Q_pu, f_Hz]`\n",
        "  - `Y_Y` shaped `(N, 8)` = `[Re(Ydd) Im(Ydd) Re(Ydq) Im(Ydq) Re(Yqd) Im(Yqd) Re(Yqq) Im(Yqq)]`\n",
        "- Files were saved with MATLAB `-v7.3` (HDF5). The loader supports both v7.3 (via `h5py`) and v7 (via `scipy.io.loadmat`).\n",
        "\n",
        "Feel free to modify the file paths in the next cell to match your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mfg6yASmJ_mY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import h5py\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== Config (train + test) ==============\n",
        "TRAIN_FILE_PATHS = [\n",
        "    \"gfmi1_impedance_dataset.mat\",\n",
        "    \"gfmi2_impedance_dataset.mat\",\n",
        "    \"gfmi3_impedance_dataset.mat\",\n",
        "    \"gfli1_impedance_dataset.mat\",\n",
        "    \"gfli2_impedance_dataset.mat\",\n",
        "    \"gfli3_impedance_dataset.mat\",\n",
        "]\n",
        "\n",
        "TEST_FILE_PATHS = [\n",
        "    \"gfmi1_test_impedance_dataset.mat\",\n",
        "    \"gfmi2_test_impedance_dataset.mat\",\n",
        "    \"gfmi3_test_impedance_dataset.mat\",\n",
        "    \"gfli1_test_impedance_dataset.mat\",\n",
        "    \"gfli2_test_impedance_dataset.mat\",\n",
        "    \"gfli3_test_impedance_dataset.mat\",\n",
        "]\n",
        "\n",
        "NBINS = 100\n",
        "# Output\n",
        "OUT_IMG = f\"bode_Ydd_kmeans_lines.png\"\n",
        "OUT_SUMMARY = f\"cluster_summary_Ydd.csv\"\n",
        "LABEL_KEY = 'Y_Y'"
      ],
      "metadata": {
        "id": "AgtobjnLKXR7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== IO helpers ==============\n",
        "\n",
        "def _fix_shape(arr, expected_cols=None):\n",
        "    \"\"\"\n",
        "    Ensure 2D shape. If expected_cols is given, transpose when needed.\n",
        "    \"\"\"\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim == 1:\n",
        "        arr = arr.reshape(-1, 1)\n",
        "    if expected_cols is not None:\n",
        "        if arr.shape[1] != expected_cols and arr.shape[0] == expected_cols:\n",
        "            arr = arr.T\n",
        "    return arr\n",
        "\n",
        "def _extract_from_h5(f, label_key=\"Y_Y\"):\n",
        "    \"\"\"\n",
        "    Try to extract X (4 cols) and Y (8 cols) from an h5py.File handle.\n",
        "    Expected structure:\n",
        "      /Dataset/X, /Dataset/Y_Y (or label_key)\n",
        "    Fallbacks:\n",
        "      /X, /Y_Y at root.\n",
        "    \"\"\"\n",
        "    # normalize key candidates\n",
        "    def get_node(g, key):\n",
        "        return g[key] if key in g else None\n",
        "\n",
        "    # 1) common path: group 'Dataset'\n",
        "    g = get_node(f, \"Dataset\")\n",
        "    if g is None:\n",
        "        # try lowercase\n",
        "        g = get_node(f, \"dataset\")\n",
        "\n",
        "    if g is not None:\n",
        "        X = _fix_shape(g[\"X\"][()], expected_cols=4)\n",
        "        Ysrc = g[label_key] if label_key in g else g.get(\"Y_Y\")\n",
        "        if Ysrc is None:\n",
        "            raise KeyError(\"Neither label_key nor 'Y_Y' found under /Dataset\")\n",
        "        Y = _fix_shape(Ysrc[()], expected_cols=8)\n",
        "        return X, Y\n",
        "\n",
        "    # 2) fallback: variables at file root\n",
        "    Xnode = get_node(f, \"X\")\n",
        "    Ynode = get_node(f, label_key) or get_node(f, \"Y_Y\")\n",
        "    if Xnode is None or Ynode is None:\n",
        "        raise KeyError(\"Could not find X and Y in HDF5 file.\")\n",
        "    X = _fix_shape(Xnode[()], expected_cols=4)\n",
        "    Y = _fix_shape(Ynode[()], expected_cols=8)\n",
        "    return X, Y\n",
        "\n",
        "def _extract_from_mat(d, label_key=\"Y_Y\"):\n",
        "    \"\"\"\n",
        "    Extract from dict returned by scipy.io.loadmat.\n",
        "    Support:\n",
        "      - struct Dataset with fields X, Y_Y (or label_key)\n",
        "      - top-level X, Y_Y\n",
        "    \"\"\"\n",
        "    # Remove MATLAB meta-keys\n",
        "    d2 = {k: v for k, v in d.items() if not k.startswith(\"__\")}\n",
        "    # struct-like 'Dataset'\n",
        "    if \"Dataset\" in d2:\n",
        "        G = d2[\"Dataset\"]\n",
        "        # If it's a numpy void (structured array)\n",
        "        if hasattr(G, \"dtype\") and G.dtype.names:\n",
        "            fields = G.dtype.names\n",
        "            def get_field(name):\n",
        "                if name in fields:\n",
        "                    val = G[name]\n",
        "                    # matlab structs often come as 1x1 arrays\n",
        "                    val = np.array(val).squeeze()\n",
        "                    return val\n",
        "                return None\n",
        "            X = _fix_shape(get_field(\"X\"), expected_cols=4)\n",
        "            Yraw = get_field(label_key) if get_field(label_key) is not None else get_field(\"Y_Y\")\n",
        "            if Yraw is None:\n",
        "                raise KeyError(\"Neither label_key nor 'Y_Y' found in Dataset struct.\")\n",
        "            Y = _fix_shape(Yraw, expected_cols=8)\n",
        "            return X, Y\n",
        "\n",
        "    # top-level\n",
        "    X = d2.get(\"X\", None)\n",
        "    Y = d2.get(label_key, None) or d2.get(\"Y_Y\", None)\n",
        "    if X is None or Y is None:\n",
        "        raise KeyError(\"Could not find X and Y in MAT file (top-level).\")\n",
        "    X = _fix_shape(X, expected_cols=4)\n",
        "    Y = _fix_shape(Y, expected_cols=8)\n",
        "    return X, Y\n",
        "\n",
        "def load_dataset_mat(path, label_key=\"Y_Y\"):\n",
        "    \"\"\"\n",
        "    Load one .mat dataset and return a dict:\n",
        "      {\n",
        "        'X': (N,4),\n",
        "        'Y': (N,8),\n",
        "        'family': 'GFMI'|'GFLI',\n",
        "        'ibr': <filename stem>,\n",
        "        'is_test': True|False\n",
        "      }\n",
        "    \"\"\"\n",
        "    path = Path(path)\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "\n",
        "    lower = path.name.lower()\n",
        "    family = \"GFMI\" if \"gfmi\" in lower else \"GFLI\"\n",
        "    is_test = \"_test_\" in lower\n",
        "    ibr = path.stem  # e.g., gfli2_impedance_dataset or gfli2_test_impedance_dataset\n",
        "\n",
        "    # Try HDF5 first\n",
        "    try:\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            X, Y = _extract_from_h5(f, label_key=label_key)\n",
        "            return {\"X\": X, \"Y\": Y, \"family\": family, \"ibr\": ibr, \"is_test\": is_test}\n",
        "    except Exception as e_h5:\n",
        "        # Fallback to old MAT format\n",
        "        try:\n",
        "            d = loadmat(str(path))\n",
        "            X, Y = _extract_from_mat(d, label_key=label_key)\n",
        "            return {\"X\": X, \"Y\": Y, \"family\": family, \"ibr\": ibr, \"is_test\": is_test}\n",
        "        except Exception as e_mat:\n",
        "            raise Exception(f\"Error loading {path}: h5py-> {e_h5}; loadmat-> {e_mat}\")\n"
      ],
      "metadata": {
        "id": "9FDMWM5VKcrD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Load TRAIN data =========\n",
        "loaded_tr, missing_tr = [], []\n",
        "for fp in TRAIN_FILE_PATHS:\n",
        "    try:\n",
        "        d = load_dataset_mat(fp, LABEL_KEY)\n",
        "        print(f\"[TRAIN] Loaded: {fp} -> X:{d['X'].shape}, Y:{d['Y'].shape}, family={d['family']}\")\n",
        "        loaded_tr.append((fp, d))\n",
        "    except Exception as e:\n",
        "        print(f\"[TRAIN] Error loading {fp}: {e}\")\n",
        "        missing_tr.append((fp, str(e)))\n",
        "\n",
        "# ========= Load TEST data (evaluation set) =========\n",
        "loaded_te, missing_te = [], []\n",
        "for fp in TEST_FILE_PATHS:\n",
        "    try:\n",
        "        d = load_dataset_mat(fp, LABEL_KEY)\n",
        "        print(f\"[TEST ] Loaded: {fp} -> X:{d['X'].shape}, Y:{d['Y'].shape}, family={d['family']}\")\n",
        "        loaded_te.append((fp, d))\n",
        "    except Exception as e:\n",
        "        print(f\"[TEST ] Error loading {fp}: {e}\")\n",
        "        missing_te.append((fp, str(e)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQJTap0FKg7e",
        "outputId": "40af00eb-bf13-45cf-a3a1-6ed361e03ae3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] Error loading gfmi1_impedance_dataset.mat: File not found: gfmi1_impedance_dataset.mat\n",
            "[TRAIN] Error loading gfmi2_impedance_dataset.mat: File not found: gfmi2_impedance_dataset.mat\n",
            "[TRAIN] Error loading gfmi3_impedance_dataset.mat: File not found: gfmi3_impedance_dataset.mat\n",
            "[TRAIN] Error loading gfli1_impedance_dataset.mat: File not found: gfli1_impedance_dataset.mat\n",
            "[TRAIN] Error loading gfli2_impedance_dataset.mat: File not found: gfli2_impedance_dataset.mat\n",
            "[TRAIN] Error loading gfli3_impedance_dataset.mat: File not found: gfli3_impedance_dataset.mat\n",
            "[TEST ] Error loading gfmi1_test_impedance_dataset.mat: File not found: gfmi1_test_impedance_dataset.mat\n",
            "[TEST ] Error loading gfmi2_test_impedance_dataset.mat: File not found: gfmi2_test_impedance_dataset.mat\n",
            "[TEST ] Error loading gfmi3_test_impedance_dataset.mat: File not found: gfmi3_test_impedance_dataset.mat\n",
            "[TEST ] Error loading gfli1_test_impedance_dataset.mat: File not found: gfli1_test_impedance_dataset.mat\n",
            "[TEST ] Error loading gfli2_test_impedance_dataset.mat: File not found: gfli2_test_impedance_dataset.mat\n",
            "[TEST ] Error loading gfli3_test_impedance_dataset.mat: File not found: gfli3_test_impedance_dataset.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Merge TRAIN only ====\n",
        "X_list, Y_list, fam_list, ibr_list = [], [], [], []\n",
        "\n",
        "for fp, d in loaded_tr:\n",
        "    X_list.append(d[\"X\"])\n",
        "    Y_list.append(d[\"Y\"])\n",
        "    fam = 0 if d[\"family\"].upper() == \"GFMI\" else 1\n",
        "    fam_list.append(np.full((d[\"X\"].shape[0],), fam, dtype=int))\n",
        "    ibr_list.append(np.array([Path(fp).stem] * d[\"X\"].shape[0], dtype=object))\n",
        "\n",
        "X_train      = np.vstack(X_list) if X_list else np.empty((0, 4))\n",
        "Y_train      = np.vstack(Y_list) if Y_list else np.empty((0, 8))\n",
        "family_train = np.concatenate(fam_list) if fam_list else np.empty((0,), dtype=int)\n",
        "ibr_train    = np.concatenate(ibr_list) if ibr_list else np.empty((0,), dtype=object)\n",
        "freq_train   = X_train[:, 3] if X_train.size else np.empty((0,))\n",
        "\n",
        "print(\"TRAIN merged shapes:\", X_train.shape, Y_train.shape)\n",
        "if family_train.size:\n",
        "    print(\"Family distribution (0=GFMI,1=GFLI):\", np.bincount(family_train))\n",
        "\n",
        "# ==== Keep TEST per-file (no merge) for later evaluation ====\n",
        "# loaded_te is kept as-is. If bạn muốn một dict tiện tra cứu:\n",
        "test_sets = {Path(fp).stem: d for fp, d in loaded_te}\n",
        "print(f\"TEST sets available: {list(test_sets.keys())}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1GcPMG3KjD5",
        "outputId": "26fbeee8-1fd5-415b-eb1b-5f93b6a6aab0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN merged shapes: (0, 4) (0, 8)\n",
            "TEST sets available: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Use TRAIN (merged) only; prepare grid & convenience aliases ====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Định danh owner để group/curves/clustering\n",
        "Vt, Pt, Qt = X_train[:, 0], X_train[:, 1], X_train[:, 2]\n",
        "owner_groups = np.array(\n",
        "    [f\"{ibr}|V{v}_P{p}_Q{q}\" for ibr, v, p, q in zip(ibr_train, Vt, Pt, Qt)],\n",
        "    dtype=object\n",
        ")\n",
        "\n",
        "print(\"Training rows (merged TRAIN):\", X_train.shape[0])\n",
        "if family_train.size:\n",
        "    print(\"Family distribution (0=GFMI,1=GFLI):\", np.bincount(family_train))\n",
        "\n",
        "# ==== Build bin grid from TRAIN ====\n",
        "NBINS = int(NBINS)  # ensure int\n",
        "pos_freq = freq_train[freq_train > 0]\n",
        "if pos_freq.size == 0:\n",
        "    raise ValueError(\"No positive frequencies found in TRAIN.\")\n",
        "lo, hi = pos_freq.min(), pos_freq.max()\n",
        "edges   = np.logspace(np.log10(lo), np.log10(hi), NBINS + 1)\n",
        "centers = np.sqrt(edges[:-1] * edges[1:])  # geometric centers\n",
        "\n",
        "# ==== |Ydd| from TRAIN ====\n",
        "# Order: [Re(Ydd), Im(Ydd), Re(Ydq), Im(Ydq), Re(Yqd), Im(Yqd), Re(Yqq), Im(Yqq)]\n",
        "Ydd_mag_train = np.hypot(Y_train[:, 0], Y_train[:, 1])\n",
        "\n",
        "# ---- (tuỳ chọn) alias cho các cell cũ có thể đang dùng tên *_all ----\n",
        "X_all      = X_train\n",
        "ibr_labels = ibr_train\n",
        "family_all = family_train\n",
        "freq_all   = freq_train\n",
        "Ydd_mag    = Ydd_mag_train\n",
        "# edges, centers giữ nguyên\n",
        "\n",
        "TRAINING_ON_ALL = True  # flag ghi chú\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "m-JXvC1TKsMh",
        "outputId": "6a6e11e8-e4ec-4567-8069-4261bdec3219"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training rows (merged TRAIN): 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No positive frequencies found in TRAIN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2883431625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpos_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreq_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreq_train\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No positive frequencies found in TRAIN.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0medges\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNBINS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No positive frequencies found in TRAIN."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Build |Ydd| (TRAIN) + helpers: log-binned median per group ====\n",
        "\n",
        "# |Ydd| từ TRAIN\n",
        "Ydd_re_tr = Y_train[:, 0]\n",
        "Ydd_im_tr = Y_train[:, 1]\n",
        "Ydd_mag_train = np.hypot(Ydd_re_tr, Ydd_im_tr)  # giữ tên biến để tương thích các cell sau\n",
        "\n",
        "def build_grid(x, nbins: int):\n",
        "    x = np.asarray(x)\n",
        "    x = x[np.isfinite(x) & (x > 0)]\n",
        "    if x.size == 0:\n",
        "        raise ValueError(\"No positive frequencies found in TRAIN.\")\n",
        "    nbins = int(nbins)\n",
        "    lo, hi = x.min(), x.max()\n",
        "    edges = np.logspace(np.log10(lo), np.log10(hi), nbins + 1)\n",
        "    centers = np.sqrt(edges[:-1] * edges[1:])  # geometric centers\n",
        "    return edges, centers\n",
        "\n",
        "def reduce_curve(freq, yvals, edges):\n",
        "    freq = np.asarray(freq); yvals = np.asarray(yvals)\n",
        "    med = np.full(len(edges) - 1, np.nan, dtype=float)\n",
        "    for i in range(len(edges) - 1):\n",
        "        m = (freq >= edges[i]) & (freq < edges[i + 1])\n",
        "        if np.any(m):\n",
        "            med[i] = np.median(yvals[m])\n",
        "    # fill NaN hai đầu để ổn định\n",
        "    s = pd.Series(med).ffill().bfill()\n",
        "    return s.values\n",
        "\n",
        "# Lưới bin từ TRAIN\n",
        "edges, centers = build_grid(freq_train, NBINS)\n",
        "\n",
        "# Alias cho cell sau (nếu còn dùng tên cũ)\n",
        "Ydd_mag = Ydd_mag_train\n",
        "freq_all = freq_train\n",
        "\n"
      ],
      "metadata": {
        "id": "zhW57vDNPxLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Build curves (TRAIN merged) ====\n",
        "# |Ydd| đã có trong Ydd_mag_train (tính từ Y_train ở cell trước)\n",
        "\n",
        "curves, owners, truth = [], [], []\n",
        "\n",
        "Vt, Pt, Qt = X_train[:, 0], X_train[:, 1], X_train[:, 2]\n",
        "df_tr = pd.DataFrame({'V': Vt, 'P': Pt, 'Q': Qt})\n",
        "df_tr['IBR'] = ibr_train\n",
        "df_tr['fam'] = family_train\n",
        "df_tr['idx'] = np.arange(len(df_tr))\n",
        "\n",
        "for (v, p, q, ibr), g in df_tr.groupby(['V', 'P', 'Q', 'IBR']):\n",
        "    idx = g['idx'].values\n",
        "    if idx.size < 20:   # bỏ nhóm quá thưa\n",
        "        continue\n",
        "    # dùng freq_train / Ydd_mag_train / edges (đều từ TRAIN)\n",
        "    vec = reduce_curve(freq_train[idx], Ydd_mag_train[idx], edges)\n",
        "    curves.append(np.log10(np.maximum(vec, 1e-12)))\n",
        "    owners.append(f\"{ibr}|V{v}_P{p}_Q{q}\")\n",
        "    truth.append(int(g['fam'].iloc[0]))\n",
        "\n",
        "curves = np.stack(curves, axis=0) if len(curves) else np.empty((0, int(NBINS)))\n",
        "owners = np.asarray(owners, dtype=object)\n",
        "truth  = np.asarray(truth, dtype=int)\n",
        "\n",
        "print(\"Curves matrix (TRAIN):\", curves.shape, \"— owners:\", len(owners))\n",
        "\n"
      ],
      "metadata": {
        "id": "jyxXuz9uO4uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Auto-select K for KMeans on TRAIN (inertia & silhouette) ====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def eval_kmeans_over_k_train(curves_tr, k_min=2, k_max=10, n_init=20, repeats=3, random_state=42):\n",
        "    \"\"\"\n",
        "    curves_tr: (n_train_samples, n_features)  -- TRAIN (đã merge)\n",
        "    Trả về:\n",
        "      - df_k: bảng inertia & silhouette theo k (trung bình qua repeats)\n",
        "      - k_best: k được chọn theo tổng hạng (silhouette ↑, inertia ↓)\n",
        "      - scaler_tr: StandardScaler đã fit trên TRAIN (để transform nhất quán về sau)\n",
        "    \"\"\"\n",
        "    if curves_tr is None or curves_tr.size == 0:\n",
        "        raise ValueError(\"curves_tr is empty. Hãy chạy cell Build curves trước.\")\n",
        "    n_samples = curves_tr.shape[0]\n",
        "    if n_samples < max(3, k_min):\n",
        "        raise ValueError(f\"Số mẫu TRAIN quá ít ({n_samples}) so với k_min={k_min}.\")\n",
        "\n",
        "    # Fit scaler trên TRAIN\n",
        "    scaler_tr = StandardScaler()\n",
        "    Xs_tr = scaler_tr.fit_transform(curves_tr)\n",
        "\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    rows = []\n",
        "    for k in range(k_min, min(k_max, n_samples - 1) + 1):\n",
        "        sil_list, inertia_list = [], []\n",
        "        for _ in range(repeats):\n",
        "            rs = int(rng.integers(0, 10_000))\n",
        "            km = KMeans(n_clusters=k, n_init=n_init, random_state=rs)\n",
        "            labels = km.fit_predict(Xs_tr)\n",
        "            inertia_list.append(km.inertia_)\n",
        "            # đôi khi silhouette có thể lỗi do dữ liệu kỳ dị; bọc try/except cho bền\n",
        "            try:\n",
        "                sil = silhouette_score(Xs_tr, labels)\n",
        "            except Exception:\n",
        "                sil = np.nan\n",
        "            sil_list.append(sil)\n",
        "\n",
        "        rows.append({\n",
        "            \"k\": k,\n",
        "            \"inertia_mean\": float(np.nanmean(inertia_list)),\n",
        "            \"silhouette_mean\": float(np.nanmean(sil_list)),\n",
        "            \"inertia_std\": float(np.nanstd(inertia_list)),\n",
        "            \"silhouette_std\": float(np.nanstd(sil_list)),\n",
        "        })\n",
        "\n",
        "    df_k = pd.DataFrame(rows)\n",
        "\n",
        "    # Heuristic: kết hợp thứ hạng (sil ↑ tốt → rank theo -sil; inertia ↓ tốt → rank theo +inertia)\n",
        "    df_k[\"rank_sil\"] = (-df_k[\"silhouette_mean\"]).rank(method=\"min\")\n",
        "    df_k[\"rank_inertia\"] = (df_k[\"inertia_mean\"]).rank(method=\"min\")\n",
        "    df_k[\"rank_sum\"] = df_k[\"rank_sil\"] + df_k[\"rank_inertia\"]\n",
        "\n",
        "    k_best = int(df_k.loc[df_k[\"rank_sum\"].idxmin(), \"k\"])\n",
        "    return df_k, k_best, scaler_tr\n",
        "\n",
        "def fit_final_kmeans_train(curves_tr, k_best, scaler_tr, n_init=50, random_state=42):\n",
        "    \"\"\"\n",
        "    Fit mô hình KMeans cuối cùng trên TRAIN và trả về:\n",
        "      - kmeans_tr: mô hình KMeans đã fit\n",
        "      - labels_tr: nhãn cụm cho TRAIN\n",
        "    \"\"\"\n",
        "    Xs_tr = scaler_tr.transform(curves_tr)\n",
        "    kmeans_tr = KMeans(n_clusters=k_best, n_init=n_init, random_state=random_state)\n",
        "    labels_tr = kmeans_tr.fit_predict(Xs_tr)\n",
        "    return kmeans_tr, labels_tr\n",
        "\n",
        "# ==== Gọi hàm với dữ liệu 'curves' (TRAIN merged) ====\n",
        "df_k, k_best, scaler_tr = eval_kmeans_over_k_train(\n",
        "    curves, k_min=2, k_max=6, n_init=20, repeats=10, random_state=42\n",
        ")\n",
        "print(\"Đề xuất k_best =\", k_best)\n",
        "display(df_k)\n",
        "\n",
        "# Fit mô hình cuối cùng trên TRAIN\n",
        "kmeans_tr, labels_tr = fit_final_kmeans_train(curves, k_best, scaler_tr, n_init=50, random_state=42)\n",
        "print(\"Cluster counts (train):\", np.bincount(labels_tr))\n",
        "\n",
        "# ==== Plot 2 chỉ số (TRAIN) ====\n",
        "fig, ax1 = plt.subplots(figsize=(6,4))\n",
        "\n",
        "# Inertia (trục trái)\n",
        "l1 = ax1.plot(df_k[\"k\"], df_k[\"inertia_mean\"],\n",
        "              marker=\"s\", color=\"tab:orange\", linestyle=\"-\",\n",
        "              markersize=9, linewidth=2, markerfacecolor=\"white\",\n",
        "              markeredgecolor=\"tab:orange\", label=\"Inertia\")\n",
        "ax1.set_xlabel(\"k\")\n",
        "ax1.set_ylabel(\"Inertia\", color=\"tab:orange\")\n",
        "ax1.tick_params(axis='y', labelcolor=\"tab:orange\")\n",
        "ax1.grid(True, ls=\":\", alpha=0.4)\n",
        "\n",
        "# Silhouette (trục phải)\n",
        "ax2 = ax1.twinx()\n",
        "l2 = ax2.plot(df_k[\"k\"], df_k[\"silhouette_mean\"],\n",
        "              marker=\"o\", color=\"tab:blue\", linestyle=\"--\",\n",
        "              markersize=9, linewidth=2, markerfacecolor=\"white\",\n",
        "              markeredgecolor=\"tab:blue\", label=\"Silhouette\")\n",
        "ax2.set_ylabel(\"Silhouette\", color=\"tab:blue\")\n",
        "ax2.tick_params(axis='y', labelcolor=\"tab:blue\")\n",
        "\n",
        "# Đánh dấu k_best\n",
        "ax1.axvline(k_best, ls=\"--\", lw=1.2, color=\"tab:gray\", alpha=0.8)\n",
        "\n",
        "# Gộp legend từ 2 trục\n",
        "lines = l1 + l2\n",
        "labels = [l.get_label() for l in lines]\n",
        "ax1.legend(lines, labels, loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig(\"kmeans_eval.png\", dpi=600, bbox_inches=\"tight\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zkYN-rCQPZ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== KMeans on standardized log10(|Ydd|) curve vectors — ALL DATA ====\n",
        "# 'curves' là ma trận CURVES từ ALL (đã build ở cell trước)\n",
        "# 'scaler_tr' và 'kmeans_tr' đã fit trên ALL ở bước auto-select K\n",
        "\n",
        "# Chuẩn hoá bằng scaler_tr (fit trên ALL)\n",
        "X_feat_all = scaler_tr.transform(curves)\n",
        "\n",
        "# Nhãn cụm cho ALL (dùng lại nếu đã có labels_tr)\n",
        "pred = kmeans_tr.predict(X_feat_all)   # hoặc: pred = labels_tr\n",
        "\n",
        "# ----- Metrics vs. family truth (0=GFMI,1=GFLI) -----\n",
        "def cluster_purity(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    n = len(y_true)\n",
        "    purity_sum = 0\n",
        "    for c in np.unique(y_pred):\n",
        "        mask = (y_pred == c)\n",
        "        if mask.any():\n",
        "            counts = np.bincount(y_true[mask].astype(int), minlength=2)\n",
        "            purity_sum += counts.max()\n",
        "    return purity_sum / n if n > 0 else np.nan\n",
        "\n",
        "# Nếu k=2, cho phép \"lật nhãn\" để so sánh trực quan với truth (0/1)\n",
        "def align_labels_binary(pred, truth):\n",
        "    acc1 = (pred == truth).mean()\n",
        "    acc2 = (1 - pred == truth).mean()\n",
        "    return pred if acc1 >= acc2 else (1 - pred)\n",
        "\n",
        "pred_aligned = align_labels_binary(pred, truth) if k_best == 2 else pred\n",
        "\n",
        "purity = cluster_purity(truth, pred_aligned)\n",
        "ari = adjusted_rand_score(truth, pred)\n",
        "\n",
        "print(f\"[ALL] Purity vs family: {purity:.4f} | ARI: {ari:.4f} | curves = {len(pred)}\")\n"
      ],
      "metadata": {
        "id": "bpgavpRAQKiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 10. Dự đoán cụm cho IBR mới (1 OP, ~10 tần số) ====\n",
        "\n",
        "NEW_FILE = \"gfli_TEST_cluster_impedance_dataset.mat\"  # đổi tên nếu cần\n",
        "\n",
        "# 1) Load file mới (dùng lại loader đã viết)\n",
        "d_new = load_dataset_mat(NEW_FILE, LABEL_KEY)\n",
        "X_new, Y_new = d_new[\"X\"], d_new[\"Y\"]\n",
        "\n",
        "# 2) Lấy tần số & |Ydd|\n",
        "freq_new = X_new[:, 3]\n",
        "Ydd_mag_new = np.hypot(Y_new[:, 0], Y_new[:, 1])  # |Ydd| = sqrt(Re^2 + Im^2)\n",
        "\n",
        "# 3) Đưa đường cong về cùng lưới bins 'edges' của TRAIN (đÃ có ở Cell 7)\n",
        "vec_new = reduce_curve(freq_new, Ydd_mag_new, edges)\n",
        "curve_new = np.log10(np.maximum(vec_new, 1e-12))[None, :]  # shape (1, NBINS)\n",
        "\n",
        "# 4) Chuẩn hoá & dự đoán cụm\n",
        "try:\n",
        "    X_feat_new = scaler_tr.transform(curve_new)          # scaler_tr phải được fit ở phần TRAIN\n",
        "    cluster_id = int(kmeans_tr.predict(X_feat_new)[0])   # kmeans_tr đã fit ở phần TRAIN\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\n",
        "        \"Thiếu scaler_tr/kmeans_tr. Hãy chắc rằng bạn đã fit mô hình ở phần TRAIN, \"\n",
        "        \"ví dụ:\\n\"\n",
        "        \"  scaler_tr = StandardScaler().fit(curves)\\n\"\n",
        "        \"  kmeans_tr = KMeans(n_clusters=k_best, random_state=0).fit(scaler_tr.transform(curves))\"\n",
        "    ) from e\n",
        "\n",
        "# 5) (Tuỳ chọn) độ tin cậy sơ bộ: khoảng cách tới tâm cụm\n",
        "dists = kmeans_tr.transform(X_feat_new)[0]   # khoảng cách đến tất cả centroids\n",
        "# In ra distance tới TẤT CẢ các cụm\n",
        "print(\"Khoảng cách tới từng cụm (Euclid, sau chuẩn hoá):\")\n",
        "for k, d in enumerate(dists):\n",
        "    print(f\"  cluster {k}: {d:.6f}\")\n",
        "\n",
        "# (tuỳ chọn) in theo thứ tự gần → xa\n",
        "order = np.argsort(dists)\n",
        "print(\"Thứ tự gần → xa:\")\n",
        "for k in order:\n",
        "    print(f\"  cluster {int(k)}: {float(dists[k]):.6f}\")\n",
        "\n",
        "best_dist = float(dists[cluster_id])\n",
        "second_best = float(np.partition(dists, 1)[1]) if len(dists) > 1 else np.nan\n",
        "margin = second_best - best_dist if np.isfinite(second_best) else np.nan\n",
        "\n",
        "print(f\"[NEW] {NEW_FILE} -> cụm dự đoán = {cluster_id}\")\n",
        "print(f\"Khoảng cách tới tâm cụm tốt nhất = {best_dist:.4f}, margin (2nd-best - best) = {margin:.4f}\")\n",
        "\n",
        "# 6) (Tuỳ chọn) Suy ra family giả định (GFMI/GFLI) dựa trên “đa số” của cụm trong TRAIN\n",
        "#     Điều này giúp gán nhãn nghĩa cho cụm (nếu k=2 hoặc bạn muốn biết cụm nghiêng về họ nào).\n",
        "try:\n",
        "    # 'pred' và 'truth' đã có ở Cell 9 (pred = labels của TRAIN curves, truth = 0:GFMI,1:GFLI)\n",
        "    cluster_to_family = {}\n",
        "    for c in np.unique(pred):\n",
        "        mask = (pred == c)\n",
        "        if mask.any():\n",
        "            counts = np.bincount(truth[mask].astype(int), minlength=2)\n",
        "            cluster_to_family[int(c)] = int(np.argmax(counts))  # 0 hoặc 1\n",
        "    fam_pred = cluster_to_family.get(cluster_id, None)\n",
        "    if fam_pred is not None:\n",
        "        fam_name = \"GFMI (0)\" if fam_pred == 0 else \"GFLI (1)\"\n",
        "        print(f\"-> Cụm {cluster_id} đa số thuộc family: {fam_name}\")\n",
        "except NameError:\n",
        "    # nếu bạn chưa tính 'pred'/'truth' ở Cell 9 thì bỏ qua phần này\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "ldDbhl6Zywhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Plot Bode lines — ALL curves colored by ALL clusters ====\n",
        "labels_all = pred_aligned if (k_best == 2) else pred\n",
        "assert curves.shape[0] == len(labels_all) == len(owners), \"Mismatch lengths!\"\n",
        "\n",
        "# Chọn colormap rời rạc theo số cụm\n",
        "uniq = np.unique(labels_all)\n",
        "if len(uniq) <= 10:\n",
        "    cmap = plt.get_cmap('tab10', len(uniq))\n",
        "elif len(uniq) <= 20:\n",
        "    cmap = plt.get_cmap('tab20', len(uniq))\n",
        "else:\n",
        "    cmap = plt.get_cmap('gist_ncar', len(uniq))\n",
        "color_map = {int(c): cmap(i) for i, c in enumerate(uniq)}\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "for i in range(curves.shape[0]):\n",
        "    cid = int(labels_all[i])\n",
        "    y = np.power(10.0, curves[i])  # curves đang là log10(|Ydd|)\n",
        "    plt.plot(centers, y, linewidth=1.5, alpha=0.7, color=color_map[cid])\n",
        "\n",
        "plt.xscale('log'); plt.yscale('log')\n",
        "plt.grid(True, which='both', ls=':', alpha=0.4)\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('|Ydd|')\n",
        "plt.title(f'|Ydd| Bode lines — ALL data (k={k_best})')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_IMG, dpi=600, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print('Saved figure ->', OUT_IMG)\n",
        "\n"
      ],
      "metadata": {
        "id": "ht3RrBUbQaU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Summary CSV — ALL DATA ====\n",
        "labels_all = pred_aligned if (k_best == 2) else pred\n",
        "\n",
        "assert len(owners) == len(truth) == len(labels_all), \"Length mismatch in summary inputs.\"\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    'owner': owners,                                   # IBR|V..._P..._Q...\n",
        "    'family_true': np.where(truth == 0, 'GFMI', 'GFLI'),\n",
        "    'family_id': truth.astype(int),\n",
        "    'cluster': labels_all.astype(int),\n",
        "})\n",
        "\n",
        "summary.to_csv(OUT_SUMMARY, index=False)\n",
        "print('Saved summary ->', OUT_SUMMARY)\n",
        "summary.head()"
      ],
      "metadata": {
        "id": "avtcv57mQwkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Per-cluster FNN (MLPRegressor) — TRAIN ONLY ====\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# --------- 0) Config ----------\n",
        "# 8 outputs: Re/Im của Ydd, Ydq, Yqd, Yqq (đã có đúng thứ tự ở Y_train)\n",
        "target_cols = slice(0, 8)\n",
        "\n",
        "# Kiến trúc mặc định nếu gặp cụm không nằm trong map\n",
        "DEFAULT_HIDDEN = (32, 32)\n",
        "\n",
        "# Map kiến trúc theo cụm: 0 = GFMI; 1,2 = GFLI\n",
        "HIDDEN_BY_CLUSTER = {\n",
        "    0: (4, 8),     # GFMI\n",
        "    1: (32, 32),   # GFLI\n",
        "    2: (32, 32),   # GFLI\n",
        "}\n",
        "\n",
        "max_iter = 1200\n",
        "random_state = 42\n",
        "\n",
        "def make_regressor(hidden=DEFAULT_HIDDEN, maxit=max_iter, rs=random_state):\n",
        "    \"\"\"\n",
        "    Pipeline hồi quy multi-output cho mỗi cụm:\n",
        "      - Chuẩn hoá X bằng StandardScaler\n",
        "      - MLPRegressor multi-output (Y có 8 cột)\n",
        "    \"\"\"\n",
        "    return Pipeline([\n",
        "        (\"x_scaler\", StandardScaler()),\n",
        "        (\"mlp\", MLPRegressor(\n",
        "            hidden_layer_sizes=hidden,\n",
        "            activation=\"relu\",\n",
        "            solver=\"adam\",\n",
        "            max_iter=maxit,\n",
        "            random_state=rs,\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "def get_hidden_for_cluster(c: int):\n",
        "    \"\"\"Trả về kiến trúc hidden phù hợp cho cluster c.\"\"\"\n",
        "    return HIDDEN_BY_CLUSTER.get(int(c), DEFAULT_HIDDEN)\n"
      ],
      "metadata": {
        "id": "ndwAmmyIrImL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- 1) Gán nhãn cụm CHO TRAIN theo owner ----------\n",
        "# Dùng nhãn cụm đã fit/ chọn ở cell 9: labels_tr (k-means trên 'curves')\n",
        "labels_train = labels_tr  # <-- thống nhất với cell 9\n",
        "\n",
        "# Ánh xạ owner (chuỗi \"ibr|V.._P.._Q..\") -> id cụm\n",
        "owner_to_cluster_train = {own: int(lbl) for own, lbl in zip(owners, labels_train)}\n",
        "\n",
        "# Build owner key cho từng hàng TRAIN\n",
        "Vt, Pt, Qt = X_train[:, 0], X_train[:, 1], X_train[:, 2]\n",
        "owner_key_train = np.array(\n",
        "    [f\"{ibr}|V{v}_P{p}_Q{q}\" for ibr, v, p, q in zip(ibr_train, Vt, Pt, Qt)],\n",
        "    dtype=object\n",
        ")\n",
        "\n",
        "# Map từng hàng TRAIN -> cụm\n",
        "cluster_id_train = np.array([owner_to_cluster_train.get(ok, -1) for ok in owner_key_train], dtype=int)\n",
        "valid_train_mask = (cluster_id_train >= 0)\n",
        "\n",
        "# Tập chỉ mục TRAIN theo cụm\n",
        "clusters = np.unique(labels_train)\n",
        "train_idx_by_cluster = {c: np.where((cluster_id_train == c) & valid_train_mask)[0] for c in clusters}\n",
        "\n",
        "print(\"Số hàng TRAIN theo cụm:\")\n",
        "for c in clusters:\n",
        "    print(f\"  Cluster {c}: {train_idx_by_cluster[c].size} rows\")\n",
        "\n",
        "# --------- 2) Gán nhãn cụm CHO CÁC BỘ TEST (dùng scaler_tr + kmeans_tr đã fit trên TRAIN) ----------\n",
        "# Lưu ý: ta làm theo MỖI FILE TEST trong dict 'test_sets' (cell 5), không merge test.\n",
        "# Trả về: test_idx_by_cluster: dict[int cluster] -> dict[str test_name] -> np.ndarray indices\n",
        "test_idx_by_cluster = {c: {} for c in clusters}\n",
        "\n",
        "for test_name, d in test_sets.items():\n",
        "    X_te = d[\"X\"]\n",
        "    Y_te = d[\"Y\"]\n",
        "    ibr_name = d[\"ibr\"]  # ví dụ: 'gfli2_test_impedance_dataset'\n",
        "\n",
        "    # Chuẩn bị nhóm theo (V,P,Q) cùng 1 IBR (file)\n",
        "    Vte, Pte, Qte, Fte = X_te[:, 0], X_te[:, 1], X_te[:, 2], X_te[:, 3]\n",
        "    df_te = pd.DataFrame({\n",
        "        \"V\": Vte,\n",
        "        \"P\": Pte,\n",
        "        \"Q\": Qte,\n",
        "        \"idx\": np.arange(X_te.shape[0])\n",
        "    })\n",
        "\n",
        "    # Xây curves owner-level trên TEST bằng edges từ TRAIN\n",
        "    # |Ydd| trên TEST:\n",
        "    Ydd_mag_test = np.hypot(Y_te[:, 0], Y_te[:, 1])\n",
        "\n",
        "    owners_te, curves_te = [], []\n",
        "    for (v, p, q), g in df_te.groupby([\"V\", \"P\", \"Q\"]):\n",
        "        idx = g[\"idx\"].values\n",
        "        if idx.size < 20:  # bỏ nhóm quá thưa, giống TRAIN\n",
        "            continue\n",
        "        vec = reduce_curve(Fte[idx], Ydd_mag_test[idx], edges)  # dùng lưới từ TRAIN\n",
        "        curves_te.append(np.log10(np.maximum(vec, 1e-12)))\n",
        "        owners_te.append(f\"{ibr_name}|V{v}_P{p}_Q{q}\")\n",
        "\n",
        "    # Dự đoán cụm cho owner TEST\n",
        "    if len(curves_te) > 0:\n",
        "        curves_test_mat = np.stack(curves_te, axis=0)\n",
        "        Xs_te_curves = scaler_tr.transform(curves_test_mat)\n",
        "        labels_te_owners = kmeans_tr.predict(Xs_te_curves)\n",
        "        owner_to_cluster_test = {own: int(lbl) for own, lbl in zip(owners_te, labels_te_owners)}\n",
        "    else:\n",
        "        owner_to_cluster_test = {}\n",
        "\n",
        "    # Map từng hàng TEST -> cụm (nếu owner không có curve hợp lệ => -1)\n",
        "    owner_key_test = np.array(\n",
        "        [f\"{ibr_name}|V{v}_P{p}_Q{q}\" for v, p, q in zip(Vte, Pte, Qte)],\n",
        "        dtype=object\n",
        "    )\n",
        "    cluster_id_test = np.array([owner_to_cluster_test.get(ok, -1) for ok in owner_key_test], dtype=int)\n",
        "    valid_test_mask = (cluster_id_test >= 0)\n",
        "\n",
        "    # Lưu chỉ mục theo cụm cho test_name\n",
        "    for c in clusters:\n",
        "        idxs = np.where((cluster_id_test == c) & valid_test_mask)[0]\n",
        "        if idxs.size > 0:\n",
        "            test_idx_by_cluster[c][test_name] = idxs\n",
        "\n",
        "# Tóm tắt\n",
        "print(\"\\nSố hàng TEST theo cụm (theo từng file):\")\n",
        "for c in clusters:\n",
        "    total_c = sum(len(v) for v in test_idx_by_cluster[c].values())\n",
        "    detail = \", \".join([f\"{k}:{len(v)}\" for k, v in test_idx_by_cluster[c].items()])\n",
        "    print(f\"  Cluster {c}: total {total_c} rows | {detail if detail else '—'}\")\n"
      ],
      "metadata": {
        "id": "yaWEhqJBUvhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# --------- 4) Train 1 FNN/cluster ----------\n",
        "models = {}\n",
        "stats_rows = []\n",
        "\n",
        "for c in clusters:\n",
        "    tr_idx = train_idx_by_cluster.get(c, np.array([], dtype=int))\n",
        "    if tr_idx.size == 0:\n",
        "        print(f\"[WARN] Cluster {c}: no TRAIN rows, skip.\")\n",
        "        continue\n",
        "\n",
        "    X_tr_c = X_train[tr_idx]\n",
        "    Y_tr_c = Y_train[tr_idx][:, target_cols]\n",
        "\n",
        "    # Lấy kiến trúc theo cụm (0 -> (4,8); 1,2 -> (32,32); mặc định -> (32,32))\n",
        "    hidden_c = get_hidden_for_cluster(c)\n",
        "\n",
        "    pipe = TransformedTargetRegressor(\n",
        "        regressor=make_regressor(hidden=hidden_c),                 # x_scaler + MLP theo cụm\n",
        "        transformer=StandardScaler(with_mean=True, with_std=True)  # scale 8 cột Y độc lập\n",
        "    )\n",
        "    pipe.fit(X_tr_c, Y_tr_c)\n",
        "    models[c] = pipe\n",
        "\n",
        "    # ---- Đánh giá nhanh trên TRAIN (per-cluster)\n",
        "    Y_tr_hat = pipe.predict(X_tr_c)\n",
        "    mae_tr = mean_absolute_error(Y_tr_c, Y_tr_hat)\n",
        "    rmse_tr = mean_squared_error(Y_tr_c, Y_tr_hat)\n",
        "\n",
        "    stats_rows.append({\n",
        "        \"cluster\": int(c), \"split\": \"train\", \"test_name\": \"\",\n",
        "        \"n\": int(tr_idx.size), \"MAE\": float(mae_tr), \"RMSE\": float(rmse_tr)\n",
        "    })\n",
        "\n",
        "    # ---- Đánh giá trên TEST (per-file)\n",
        "    te_dict = test_idx_by_cluster.get(c, {})\n",
        "    X_blocks, Y_blocks, n_total = [], [], 0\n",
        "\n",
        "    for test_name, idxs in te_dict.items():\n",
        "        X_te_c = test_sets[test_name][\"X\"][idxs]\n",
        "        Y_te_c = test_sets[test_name][\"Y\"][idxs][:, target_cols]\n",
        "\n",
        "        Y_te_hat = pipe.predict(X_te_c)\n",
        "        mae_te = mean_absolute_error(Y_te_c, Y_te_hat)\n",
        "        rmse_te = mean_squared_error(Y_te_c, Y_te_hat)\n",
        "\n",
        "        stats_rows.append({\n",
        "            \"cluster\": int(c), \"split\": \"test\", \"test_name\": test_name,\n",
        "            \"n\": int(idxs.size), \"MAE\": float(mae_te), \"RMSE\": float(rmse_te)\n",
        "        })\n",
        "\n",
        "        X_blocks.append(X_te_c)\n",
        "        Y_blocks.append(Y_te_c)\n",
        "        n_total += idxs.size\n",
        "\n",
        "    if n_total > 0:\n",
        "        X_te_all = np.vstack(X_blocks)\n",
        "        Y_te_all = np.vstack(Y_blocks)\n",
        "        Y_te_all_hat = pipe.predict(X_te_all)\n",
        "        mae_te_all = mean_absolute_error(Y_te_all, Y_te_all_hat)\n",
        "        rmse_te_all = mean_squared_error(Y_te_all, Y_te_all_hat)\n",
        "\n",
        "        stats_rows.append({\n",
        "            \"cluster\": int(c), \"split\": \"test_all\", \"test_name\": \"ALL\",\n",
        "            \"n\": int(n_total), \"MAE\": float(mae_te_all), \"RMSE\": float(rmse_te_all)\n",
        "        })\n",
        "\n",
        "# Tổng hợp bảng thống kê\n",
        "stats_df = pd.DataFrame(stats_rows).sort_values([\"cluster\", \"split\", \"test_name\"]).reset_index(drop=True)\n",
        "display(stats_df)"
      ],
      "metadata": {
        "id": "roGdKmVZQ7cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Scatter + regression line cho 8 output components (TEST, theo cụm từ cell 11) ====\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Gom toàn bộ điểm TEST hợp lệ theo đúng cụm -> dự đoán bằng models[c]\n",
        "Y_te_true_all_list = []\n",
        "Y_te_pred_all_list = []\n",
        "\n",
        "for c, mdl in models.items():\n",
        "    te_dict = test_idx_by_cluster.get(c, {})\n",
        "    for test_name, idxs in te_dict.items():\n",
        "        if idxs.size == 0:\n",
        "            continue\n",
        "        X_te = test_sets[test_name][\"X\"][idxs]\n",
        "        Y_te = test_sets[test_name][\"Y\"][idxs][:, target_cols]\n",
        "\n",
        "        Y_hat = mdl.predict(X_te)\n",
        "\n",
        "        Y_te_true_all_list.append(Y_te)\n",
        "        Y_te_pred_all_list.append(Y_hat)\n",
        "\n",
        "# Kiểm tra có dữ liệu TEST hợp lệ không\n",
        "if len(Y_te_true_all_list) == 0:\n",
        "    raise RuntimeError(\"Không có hàng TEST hợp lệ để vẽ scatter (test_idx_by_cluster rỗng).\")\n",
        "\n",
        "Y_te_true_all = np.vstack(Y_te_true_all_list)\n",
        "Y_te_pred_all = np.vstack(Y_te_pred_all_list)\n",
        "\n",
        "# Vẽ 8 đồ thị (2x4)\n",
        "names = [\n",
        "    \"Re(Ydd)\", \"Im(Ydd)\",\n",
        "    \"Re(Ydq)\", \"Im(Ydq)\",\n",
        "    \"Re(Yqd)\", \"Im(Yqd)\",\n",
        "    \"Re(Yqq)\", \"Im(Yqq)\",\n",
        "]\n",
        "n_out = Y_te_true_all.shape[1]\n",
        "assert n_out == len(names) == 8, f\"Số cột output ({n_out}) không khớp 8.\"\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for j in range(n_out):\n",
        "    y_true = Y_te_true_all[:, j]\n",
        "    y_pred = Y_te_pred_all[:, j]\n",
        "\n",
        "    ax = axes[j]\n",
        "    ax.scatter(y_true, y_pred, s=10, alpha=0.6)\n",
        "\n",
        "    # y = x\n",
        "    lo = float(min(np.min(y_true), np.min(y_pred)))\n",
        "    hi = float(max(np.max(y_true), np.max(y_pred)))\n",
        "    ax.plot([lo, hi], [lo, hi], lw=1.3)\n",
        "\n",
        "    # đường hồi quy tuyến tính (OLS) + R^2\n",
        "    if y_true.size >= 2 and np.std(y_true) > 0:\n",
        "        coef = np.polyfit(y_true, y_pred, 1)\n",
        "        reg_line = np.poly1d(coef)\n",
        "        ax.plot([lo, hi], reg_line([lo, hi]), lw=1.3, linestyle=\"--\")\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        ax.set_title(f\"{names[j]}\\nSlope={coef[0]:.3f}, Intercept={coef[1]:.3f}, R²={r2:.4f}\")\n",
        "    else:\n",
        "        ax.set_title(f\"{names[j]}\")\n",
        "\n",
        "    ax.set_xlabel(\"True\")\n",
        "    ax.set_ylabel(\"Pred\")\n",
        "    ax.grid(True, ls=\":\", alpha=0.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oUl-LAIluCKs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Overlay Re & Im (True vs Pred) cho Ydd, Ydq, Yqd, Yqq — TEST (theo cụm/owner từ cell 11) ====\n",
        "from matplotlib.lines import Line2D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# số owner tối đa mỗi cụm để vẽ (để hình không quá rối)\n",
        "N_OWNERS_PER_CLUSTER = 1\n",
        "\n",
        "# mapping vị trí các cột: (Re_idx, Im_idx)\n",
        "components = [\n",
        "    (\"Ydd\", 0, 1),\n",
        "    (\"Ydq\", 2, 3),\n",
        "    (\"Yqd\", 4, 5),\n",
        "    (\"Yqq\", 6, 7),\n",
        "]\n",
        "\n",
        "for c, mdl in models.items():\n",
        "    # Lấy các hàng TEST thuộc cụm c, theo từng file test\n",
        "    te_dict = test_idx_by_cluster.get(c, {})\n",
        "    if not te_dict:\n",
        "        print(f\"[INFO] Cluster {c}: không có TEST rows.\")\n",
        "        continue\n",
        "\n",
        "    # Chọn tối đa N_OWNERS_PER_CLUSTER owner (V,P,Q, IBR) để vẽ\n",
        "    owners_selected = []  # list of tuples: (owner_key, test_name, idxs_local)\n",
        "    for test_name, idxs in te_dict.items():\n",
        "        if len(owners_selected) >= N_OWNERS_PER_CLUSTER:\n",
        "            break\n",
        "        X_te = test_sets[test_name][\"X\"][idxs]\n",
        "        Y_te = test_sets[test_name][\"Y\"][idxs]\n",
        "        ibr_name = test_sets[test_name][\"ibr\"]\n",
        "\n",
        "        # group theo (V,P,Q) bên trong những hàng đã thuộc cụm c\n",
        "        Vte, Pte, Qte = X_te[:, 0], X_te[:, 1], X_te[:, 2]\n",
        "        df_te = pd.DataFrame({\n",
        "            \"V\": Vte, \"P\": Pte, \"Q\": Qte,\n",
        "            \"idx_local\": np.arange(X_te.shape[0])  # chỉ số local trong khối đã lọc theo cụm\n",
        "        })\n",
        "\n",
        "        for (v, p, q), g in df_te.groupby([\"V\", \"P\", \"Q\"]):\n",
        "            if len(owners_selected) >= N_OWNERS_PER_CLUSTER:\n",
        "                break\n",
        "            local_idx = g[\"idx_local\"].values\n",
        "            if local_idx.size < 20:  # giống điều kiện ở TRAIN/TEST trước\n",
        "                continue\n",
        "            owner_key = f\"{ibr_name}|V{v}_P{p}_Q{q}\"\n",
        "            owners_selected.append((owner_key, test_name, local_idx))\n",
        "\n",
        "    if not owners_selected:\n",
        "        print(f\"[INFO] Cluster {c}: không chọn được owner đủ điểm (>=20) để vẽ.\")\n",
        "        continue\n",
        "\n",
        "    # Figure cho phần thực (Re) — 2x2\n",
        "    fig_re, axes_re = plt.subplots(2, 2, figsize=(5, 4),\n",
        "                               gridspec_kw={'hspace': 0.7, 'wspace': 0.3})\n",
        "    axes_re = axes_re.ravel()\n",
        "    # Figure cho phần ảo (Im) — 2x2\n",
        "    fig_im, axes_im = plt.subplots(2, 2, figsize=(5, 4),\n",
        "                               gridspec_kw={'hspace': 0.7, 'wspace': 0.3})\n",
        "    axes_im = axes_im.ravel()\n",
        "    # proxy handles cho legend ngắn gọn\n",
        "    legend_proxies = [\n",
        "    Line2D([0], [0], color='tab:blue', lw=2.5, ls='-',  label='TRUE'),\n",
        "    Line2D([0], [0], color='tab:orange', lw=2.5, ls='--', label='PRED'),\n",
        "    ]\n",
        "\n",
        "    for owner_key, test_name, local_idx in owners_selected:\n",
        "        X_te_block = test_sets[test_name][\"X\"][te_dict[test_name]]  # block tất cả rows của cụm trong file đó\n",
        "        Y_te_block = test_sets[test_name][\"Y\"][te_dict[test_name]]\n",
        "\n",
        "        # Lấy phần của owner trong block (dùng chỉ số local)\n",
        "        X_owner = X_te_block[local_idx, :]\n",
        "        Y_owner_true = Y_te_block[local_idx, :]\n",
        "\n",
        "        f = X_owner[:, 3]\n",
        "        Y_owner_pred = mdl.predict(X_owner)\n",
        "        short_owner = owner_key.split('|', 1)[-1].replace('_', ' ').replace('=', '')\n",
        "        if 'gfmi' in owner_key.lower(): tag = 'GFMI'\n",
        "        elif 'gfli' in owner_key.lower(): tag = 'GFLI'\n",
        "        else: tag = owner_key.split('|',1)[0]\n",
        "        owner_tag = f\"{tag} | {short_owner}\"\n",
        "\n",
        "        for comp_i, (name, re_i, im_i) in enumerate(components):\n",
        "            # --- Re ---\n",
        "            re_true = Y_owner_true[:, re_i]\n",
        "            re_pred = Y_owner_pred[:, re_i]\n",
        "            curve_re_true = reduce_curve(f, re_true, edges)   # median theo bin (dùng edges từ TRAIN)\n",
        "            curve_re_pred = reduce_curve(f, re_pred, edges)\n",
        "\n",
        "            axr = axes_re[comp_i]\n",
        "            axr.semilogx(centers, curve_re_true, linewidth=3.0, alpha=0.9)\n",
        "            axr.semilogx(centers, curve_re_pred, linewidth=3.0, alpha=0.9, linestyle=\"--\")\n",
        "            axr.grid(True, which='both', ls=':', alpha=0.4)\n",
        "            axr.set_title(f\"Re({name})\")\n",
        "            axr.set_xlabel('Frequency (Hz)')\n",
        "            axr.set_ylabel('')\n",
        "            axr.legend(handles=legend_proxies, loc=\"upper right\",\n",
        "                   fontsize=8, framealpha=0.9, borderpad=0.4, handlelength=2.5)\n",
        "            axr.text(0.02, 0.98, owner_tag, transform=axr.transAxes,\n",
        "                 va='top', ha='left', fontsize=7,\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"0.7\", alpha=0.8))\n",
        "\n",
        "            # --- Im ---\n",
        "            im_true = Y_owner_true[:, im_i]\n",
        "            im_pred = Y_owner_pred[:, im_i]\n",
        "            curve_im_true = reduce_curve(f, im_true, edges)\n",
        "            curve_im_pred = reduce_curve(f, im_pred, edges)\n",
        "\n",
        "            axi = axes_im[comp_i]\n",
        "            axi.semilogx(centers, curve_im_true, linewidth=3.0, alpha=0.9)\n",
        "            axi.semilogx(centers, curve_im_pred, linewidth=3.0, alpha=0.9, linestyle=\"--\")\n",
        "            axi.grid(True, which='both', ls=':', alpha=0.4)\n",
        "            axi.set_title(f\"Im({name})\")\n",
        "            axi.set_xlabel('Frequency (Hz)')\n",
        "            axi.set_ylabel('')\n",
        "            axi.legend(handles=legend_proxies, loc=\"upper right\",\n",
        "                   fontsize=8, framealpha=0.9, borderpad=0.4, handlelength=2.5)\n",
        "\n",
        "            axi.text(0.02, 0.98, owner_tag, transform=axi.transAxes,\n",
        "                 va='top', ha='left', fontsize=7,\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"0.7\", alpha=0.8))\n",
        "\n",
        "    # Legend chung & tiêu đề cho mỗi figure\n",
        "    fig_re.savefig(f\"real_parts_cluster{c}.png\", dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "    fig_im.savefig(f\"imag_parts_cluster{c}.png\", dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4Gh7jZFVu74O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 3D surfaces for NEW IBR: TRUE • PRED • ERROR (fix V & Q, sweep P & f) ====\n",
        "# - Input true file: gfmi_TEST_impedance_dataset.mat\n",
        "# - Predict bằng model theo cụm đã xác định: cluster_id\n",
        "# - Plot 3 hình 3D (True / Pred / Error), mỗi hình 4 subplot: |Re(Ydd)|, |Re(Ydq)|, |Re(Yqd)|, |Re(Yqq)|\n",
        "# --- thêm import màu sắc ở đầu cell ---\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.colors import Normalize, TwoSlopeNorm\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "from pathlib import Path\n",
        "\n",
        "# --------- Config ----------\n",
        "TRUE_FILE = \"gfli_TEST_impedance_dataset.mat\"\n",
        "CLUSTER_FOR_NEW = int(cluster_id)  # đã xác định ở bước phân cụm cho IBR mới\n",
        "FIX_V = None   # None = auto (giá trị xuất hiện nhiều nhất trong file true)\n",
        "FIX_Q = None   # None = auto\n",
        "SWEEP_NBINS = 24        # số bin cho trục P\n",
        "DB_EPS = 1e-12\n",
        "\n",
        "# --------- Helpers ----------\n",
        "def mag_db_real(re, eps=DB_EPS):\n",
        "    \"\"\"20*log10(|Re(.)| + eps) để bỏ phần ảo theo yêu cầu.\"\"\"\n",
        "    return 20.0 * np.log10(np.abs(re) + eps)\n",
        "\n",
        "def choose_mode(vals):\n",
        "    s = pd.Series(vals).round(9).value_counts()\n",
        "    return float(s.index[0]) if len(s) else float(np.median(vals) if len(vals) else 0.0)\n",
        "\n",
        "def build_lin_edges(x, nbins):\n",
        "    x = np.asarray(x)\n",
        "    lo, hi = np.nanmin(x), np.nanmax(x)\n",
        "    if not np.isfinite(lo) or not np.isfinite(hi) or lo == hi:\n",
        "        hi = lo + 1.0\n",
        "    return np.linspace(lo, hi, int(nbins) + 1)\n",
        "\n",
        "def grid_median(freq, sweep, z, f_edges, s_edges):\n",
        "    \"\"\"Binning theo f (log edges từ TRAIN) và sweep (linear). Median trong ô; ffill/bfill mép.\"\"\"\n",
        "    Fmat = np.full((len(s_edges)-1, len(f_edges)-1), np.nan, float)\n",
        "    for i in range(len(s_edges)-1):\n",
        "        ms = (sweep >= s_edges[i]) & (sweep < s_edges[i+1])\n",
        "        if not np.any(ms):\n",
        "            continue\n",
        "        f_sub, z_sub = freq[ms], z[ms]\n",
        "        for j in range(len(f_edges)-1):\n",
        "            mf = (f_sub >= f_edges[j]) & (f_sub < f_edges[j+1])\n",
        "            if np.any(mf):\n",
        "                Fmat[i, j] = np.median(z_sub[mf])\n",
        "\n",
        "    Fmat = pd.DataFrame(Fmat).ffill(axis=0).bfill(axis=0).ffill(axis=1).bfill(axis=1).values\n",
        "    f_cent = np.sqrt(f_edges[:-1] * f_edges[1:])   # log centers\n",
        "    s_cent = 0.5 * (s_edges[:-1] + s_edges[1:])\n",
        "    return f_cent, s_cent, Fmat\n",
        "\n",
        "def log_ticks(minv, maxv):\n",
        "    a, b = np.floor(np.log10(minv)), np.ceil(np.log10(maxv))\n",
        "    vals = 10.0 ** np.arange(a, b+1)\n",
        "    return np.log10(vals), [f\"$10^{{{int(x)}}}$\" for x in range(int(a), int(b)+1)]\n",
        "\n",
        "# === THAY THẾ HÀM VẼ ===\n",
        "def _shared_norm(surfaces, mode=\"mag\"):\n",
        "    \"\"\"Chuẩn hoá chung cho 4 subplot trong 1 hàng.\"\"\"\n",
        "    vals = []\n",
        "    for k in [\"Ydd\", \"Ydq\", \"Yqd\", \"Yqq\"]:\n",
        "        if k in surfaces and surfaces[k] is not None:\n",
        "            Z = surfaces[k][2]\n",
        "            if Z.size:\n",
        "                vals.append(Z[np.isfinite(Z)])\n",
        "    if not vals:\n",
        "        return None\n",
        "    vmin = min(v.min() for v in vals)\n",
        "    vmax = max(v.max() for v in vals)\n",
        "    if mode == \"err\":\n",
        "        v = max(abs(vmin), abs(vmax))\n",
        "        return TwoSlopeNorm(vmin=-v, vcenter=0.0, vmax=v)\n",
        "    return Normalize(vmin=vmin, vmax=vmax)\n",
        "\n",
        "def plot_surface_row(axarr, surfaces, row_title,\n",
        "                     zlabel=\"Magnitude (dB)\", ylabel=\"P (pu)\", mode=\"mag\"):\n",
        "    \"\"\"\n",
        "    mode='mag' dùng colormap tuần tự (viridis), mode='err' dùng colormap phân kỳ (coolwarm, center=0).\n",
        "    \"\"\"\n",
        "    comps = [\"Ydd\", \"Ydq\", \"Yqd\", \"Yqq\"]\n",
        "    norm = _shared_norm(surfaces, mode=mode)\n",
        "    cmap = mpl.colormaps['viridis'] if mode != 'err' else mpl.colormaps['coolwarm']\n",
        "    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
        "\n",
        "    for j, name in enumerate(comps):\n",
        "        ax = axarr[j]\n",
        "        if name not in surfaces or surfaces[name] is None:\n",
        "            ax.set_title(f\"{name} (no data)\", fontsize=10); continue\n",
        "\n",
        "        f_cent, s_cent, Z = surfaces[name]\n",
        "        Xg, Yg = np.meshgrid(np.log10(f_cent), s_cent)\n",
        "\n",
        "        # tô màu theo Z\n",
        "        facecolors = cmap(norm(Z))\n",
        "        ax.plot_surface(Xg, Yg, Z,\n",
        "                        facecolors=facecolors,\n",
        "                        rstride=1, cstride=1,\n",
        "                        linewidth=0, antialiased=True, shade=False, alpha=1.0)\n",
        "\n",
        "        # (tuỳ chọn) lưới mảnh giống hình paper\n",
        "        ax.plot_wireframe(Xg, Yg, Z, rstride=3, cstride=3, color=\"k\", linewidth=0.15, alpha=0.25)\n",
        "\n",
        "        # trục/tick\n",
        "        xtick, xtlbl = log_ticks(f_cent.min(), f_cent.max())\n",
        "        ax.set_xticks(xtick); ax.set_xticklabels(xtlbl, fontsize=8)\n",
        "        ax.set_xlabel('Frequency (Hz)', labelpad=4)\n",
        "        ax.set_ylabel(ylabel, labelpad=4)\n",
        "        ax.set_zlabel(zlabel, labelpad=6)\n",
        "        ax.set_title(name, fontsize=14)\n",
        "        ax.view_init(elev=25, azim=-60)          # góc nhìn dễ đọc\n",
        "        ax.set_box_aspect((1.2, 1.0, 0.6))       # tỉ lệ khối đẹp hơn\n",
        "\n",
        "    # # colorbar dùng chung cho cả hàng\n",
        "    # fig = axarr[0].figure\n",
        "    # cbar = fig.colorbar(mappable, ax=axarr.ravel().tolist(),\n",
        "    #                     fraction=0.03, pad=0.02)   # ← thay cho shrink\n",
        "    # cbar.set_label(zlabel, rotation=90)\n",
        "    # fig.suptitle(row_title, y=0.98, fontsize=11)\n",
        "\n",
        "\n",
        "# --------- Load true file & filter V,Q ----------\n",
        "d_true = load_dataset_mat(TRUE_FILE, LABEL_KEY)\n",
        "X_true, Y_true = d_true[\"X\"], d_true[\"Y\"]\n",
        "V, P, Q, F = X_true[:,0], X_true[:,1], X_true[:,2], X_true[:,3]\n",
        "\n",
        "v_fix = choose_mode(V) if FIX_V is None else float(FIX_V)\n",
        "q_fix = choose_mode(Q) if FIX_Q is None else float(FIX_Q)\n",
        "mask_vq = np.isclose(V, v_fix, atol=1e-9) & np.isclose(Q, q_fix, atol=1e-9)\n",
        "\n",
        "# nếu quá ít điểm, nới điều kiện bằng cách chọn mode gần nhất giàu điểm\n",
        "if mask_vq.sum() < 80:\n",
        "    v_counts = pd.Series(V).round(9).value_counts()\n",
        "    q_counts = pd.Series(Q).round(9).value_counts()\n",
        "    for vv in v_counts.index:\n",
        "        for qq in q_counts.index:\n",
        "            m2 = np.isclose(V, float(vv), atol=1e-9) & np.isclose(Q, float(qq), atol=1e-9)\n",
        "            if m2.sum() >= 80:\n",
        "                v_fix, q_fix = float(vv), float(qq)\n",
        "                mask_vq = m2\n",
        "                break\n",
        "        if mask_vq.sum() >= 80:\n",
        "            break\n",
        "\n",
        "X_sub   = X_true[mask_vq]\n",
        "Y_sub_T = Y_true[mask_vq]     # ground truth\n",
        "P_sub   = P[mask_vq]\n",
        "F_sub   = F[mask_vq]\n",
        "\n",
        "if X_sub.size == 0:\n",
        "    raise RuntimeError(\"Không tìm thấy dữ liệu với V,Q cố định trong file true. Hãy đặt FIX_V, FIX_Q thủ công.\")\n",
        "\n",
        "print(f\"[INFO] Using V={v_fix:g}, Q={q_fix:g} | rows={X_sub.shape[0]} | cluster={CLUSTER_FOR_NEW}\")\n",
        "\n",
        "# --------- Predict with pretrained model of the detected cluster ----------\n",
        "if CLUSTER_FOR_NEW not in models:\n",
        "    raise KeyError(f\"Không có model cho cluster {CLUSTER_FOR_NEW}. Kiểm tra bước train per-cluster (Cell 13).\")\n",
        "\n",
        "Y_sub_P = models[CLUSTER_FOR_NEW].predict(X_sub)\n",
        "\n",
        "# --------- Build surfaces (median bins) for True / Pred / Error (Re-only, in dB) ----------\n",
        "# Mapping cột: [Re(Ydd), Im(Ydd), Re(Ydq), Im(Ydq), Re(Yqd), Im(Yqd), Re(Yqq), Im(Yqq)]\n",
        "def build_surfaces_real_re_db(Ymat, F_vec, P_vec, f_edges, p_edges):\n",
        "    comps = {\n",
        "        \"Ydd\": mag_db_real(Ymat[:, 0]),\n",
        "        \"Ydq\": mag_db_real(Ymat[:, 2]),\n",
        "        \"Yqd\": mag_db_real(Ymat[:, 4]),\n",
        "        \"Yqq\": mag_db_real(Ymat[:, 6]),\n",
        "    }\n",
        "    out = {}\n",
        "    for name, z in comps.items():\n",
        "        f_cent, p_cent, Z = grid_median(F_vec, P_vec, z, f_edges=f_edges, s_edges=p_edges)\n",
        "        out[name] = (f_cent, p_cent, Z)\n",
        "    return out\n",
        "\n",
        "p_edges = build_lin_edges(P_sub, SWEEP_NBINS)   # sweep P theo tuyến tính\n",
        "f_edges = edges                                  # dùng lưới f (log) đã fit từ TRAIN\n",
        "\n",
        "sur_true = build_surfaces_real_re_db(Y_sub_T, F_sub, P_sub, f_edges, p_edges)\n",
        "sur_pred = build_surfaces_real_re_db(Y_sub_P, F_sub, P_sub, f_edges, p_edges)\n",
        "\n",
        "# Error (dB) = Pred_dB - True_dB\n",
        "sur_err = {}\n",
        "for key in sur_true.keys():\n",
        "    f_cent_t, p_cent_t, Zt = sur_true[key]\n",
        "    f_cent_p, p_cent_p, Zp = sur_pred[key]\n",
        "    # (hai lưới giống nhau vì cùng edges)\n",
        "    sur_err[key] = (f_cent_t, p_cent_t, Zp - Zt)\n",
        "\n",
        "# --------- Plot: 3 figures × 4 subplots (|Re(.)| dB) ----------\n",
        "fig1, axes1 = plt.subplots(1, 4, figsize=(15, 4.6), subplot_kw={\"projection\": \"3d\"})\n",
        "plot_surface_row(axes1, sur_true,\n",
        "                 row_title=f\"TRUE • |Re(Y·)| (dB) • V={v_fix:g}, Q={q_fix:g} • Cluster {CLUSTER_FOR_NEW}\",\n",
        "                 zlabel=\"|Re(Y)| (dB)\", ylabel=\"P (pu)\", mode=\"mag\")\n",
        "fig1.subplots_adjust(left=0.05, right=0.97, top=0.88, bottom=0.14, wspace=0.10)\n",
        "plt.show()\n",
        "\n",
        "fig2, axes2 = plt.subplots(1, 4, figsize=(15, 4.6), subplot_kw={\"projection\": \"3d\"})\n",
        "plot_surface_row(axes2, sur_pred,\n",
        "                 row_title=f\"PRED • |Re(Y·)| (dB) • V={v_fix:g}, Q={q_fix:g} • Cluster {CLUSTER_FOR_NEW}\",\n",
        "                 zlabel=\"|Re(Y)| (dB)\", ylabel=\"P (pu)\", mode=\"mag\")\n",
        "fig2.subplots_adjust(left=0.05, right=0.97, top=0.88, bottom=0.14, wspace=0.10)\n",
        "plt.show()\n",
        "\n",
        "fig3, axes3 = plt.subplots(1, 4, figsize=(15, 4.6), subplot_kw={\"projection\": \"3d\"})\n",
        "plot_surface_row(axes3, sur_err,\n",
        "                 row_title=f\"ERROR (Pred − True) • |Re(Y·)| (dB) • V={v_fix:g}, Q={q_fix:g} • Cluster {CLUSTER_FOR_NEW}\",\n",
        "                 zlabel=\"Δ|Re(Y)| (dB)\", ylabel=\"P (pu)\", mode=\"err\")\n",
        "fig3.subplots_adjust(left=0.05, right=0.97, top=0.88, bottom=0.14, wspace=0.10)\n",
        "\n",
        "\n",
        "# --- folder & tên file ---\n",
        "outdir = Path(\"exports_plots\")\n",
        "outdir.mkdir(exist_ok=True)\n",
        "tag = f\"V{v_fix:g}_Q{q_fix:g}_cluster{CLUSTER_FOR_NEW}\"\n",
        "\n",
        "# --- thông số xuất ---\n",
        "PNG_DPI = 600  # 300–600 là chuẩn in\n",
        "SAVE_KW = dict(bbox_inches=\"tight\", pad_inches=0.02, facecolor=\"white\")\n",
        "\n",
        "# TRUE\n",
        "fig1.savefig(outdir / f\"TRUE_ReY_{tag}.png\", dpi=PNG_DPI, **SAVE_KW)\n",
        "fig1.savefig(outdir / f\"TRUE_ReY_{tag}.pdf\", **SAVE_KW)   # vector cho in ấn\n",
        "\n",
        "# PRED\n",
        "fig2.savefig(outdir / f\"PRED_ReY_{tag}.png\", dpi=PNG_DPI, **SAVE_KW)\n",
        "fig2.savefig(outdir / f\"PRED_ReY_{tag}.pdf\", **SAVE_KW)\n",
        "\n",
        "# ERROR\n",
        "fig3.savefig(outdir / f\"ERROR_ReY_{tag}.png\", dpi=PNG_DPI, **SAVE_KW)\n",
        "fig3.savefig(outdir / f\"ERROR_ReY_{tag}.pdf\", **SAVE_KW)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XnCtpFmzwdWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRj-wKoS8J6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}